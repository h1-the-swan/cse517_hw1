Jason Portenoy | CSE 517 Natural Language Processing | Assignment 1
January 20, 2016

I used interpolated n-gram models, from unigram up to 5-gram. For each n-gram model I stored the conditional frequency distribution of each gram---for example, the trigram file contains a key-value store, the key of which is a two-character sequence, and the value of which is a second key-value store of the final character and the observed frequency of that trigram in the training data. Given a history, I calculate the probability (for each n-gram model) for each character in the Unicode BMP alphabet using the observed frequency plus additive smoothing (adding a small lambda 0.0001 to each count, and lambda times the size of the alphabet to the denominator). I interpolate the models by (for each character) adding together the probabilities for each model times a weight, so that all of the weights sum to one (the weights I used were, from unigram to 5-gram: [0.01, 0.04, 0.15, 0.3, 0.5]). I tuned the parameters (lambda and model weights) by hand using development data. In the case that the history is empty, I use a probability distribution just based on the initial characters I've seen in each sentence in my training data. I wasn't happy with the way it was generating characters, so I limited each generation to be in the top fifty most likely characters at any given time. In the case that the history is shorter than the highest-order n-gram model can handle, I only use the appropriate n-gram models, and transfer the interpolating weights of the unused models onto the highest-order model I used. The data I used to train comes from Tatoeba (<https://tatoeba.org>), a collaborative and open collection of sentences and translation. The corpus contains 4548359 user-contributed sentences in 273 languages. I split these into training and development sets (85% went to training). To train the model, I fed each sentence into the counter, ending with a stop character (U+0003). I used Python as the main programming language. I used the Natural Language Toolkit (NLTK) to do n-gram splitting, and also used their data structures FreqDist and ConditionalFreqDist (these are modified Python dictionaries). I used numpy to select characters according to my probability distributions. I used pandas for some basic data manipulation, and scikit-learn to split my data into training and development sets. I received some help from Professor Smith (in office hours), and had basic conversations about strategies with classmates Lucy Wang, Lovenoor Aulck, and Li Zheng.

Additional note: I went down a bit of a hole trying to implement a log-linear model into which I could feed arbitrary features. In the end, I ran into too many memory issues and didn't have enough time to get it to work, so I reverted to the n-gram model I had been working on before.
